[["index.html", "统计计算作业汇总 简介", " 统计计算作业汇总 兰敬国 2021-12-22 简介 这是统计计算平时作业汇总而成的文件, 平时的.rmd文件用Bookdown合成“书”的形式, 格式更加清晰。 为了让结构更完整，还借鉴了老师上课PPT的一些源码。 Bookdown的详细用法参见https://bookdown.org/yihui/bookdown/ "],["第1次作业解答.html", "1 第1次作业解答 1.1 Question 1.2 Answer", " 1 第1次作业解答 1.1 Question Produce 3 examples(texts,figures,tables) 1.2 Answer 1.2.1 Texts a=c(&quot;今天&quot;,&quot;吃饭&quot;) b=c(&quot;你&quot;,&quot;了吗?&quot;) d=paste0(a,b,collapse=&quot;&quot;) ## 拼接字符串a,b d ## [1] &quot;今天你吃饭了吗?&quot; 1.2.2 Figures ## 从均值为100，方差为1的正态分布中，随机生成30个数 x &lt;- rnorm(30, mean=100, sd=1) print(round(x,2)) ## [1] 97.31 100.06 99.39 100.87 100.15 100.07 100.52 99.86 102.16 98.56 ## [11] 100.52 97.42 101.16 101.76 100.29 100.94 98.96 99.15 99.29 99.30 ## [21] 100.49 99.16 99.48 100.84 99.76 100.66 100.65 99.74 101.07 99.57 ## 30个随机数的散点图 plot(x,main=&quot;散点图&quot;) ## 30个随机数的直方图 hist(x, col=rainbow(15), main=&#39;正态随机数&#39;, xlab=&#39;&#39;, ylab=&#39;频数&#39;) 1.2.3 ggplot2 ggplot2包的mpg数据集:(从1999年到2008年38款流行车型的燃油经济性数据) ，234*11的数据规模，记录了制造厂商，型号，类别，驱动程序和耗油量 cty 和hwy分别记录城市和高速公路驾驶耗油量 cyl:气缸数 displ:发动机排量 drv:驱动系统：前轮驱动(f)、后轮驱动和四轮驱动(4) class:车辆类型,如双座汽车,suv,小型汽车 fl:燃料类型 ## 用ggplot2画图 library(ggplot2) data(mpg) ## 导入mpg数据集 ggplot(data=mpg,mapping = aes(x=cty,y=hwy,color=factor(year)))+ geom_point()+stat_smooth(method = &#39;loess&#39;)+ scale_shape_manual(values = c(2,5))+ labs(y = &#39;每加仑高速公路行驶距离&#39;, x = &#39;每加仑城市公路行驶距离&#39;, title = &#39;汽车油耗与型号&#39;, size = &#39;排量&#39;, colour = &#39;车型&#39;)+ theme(plot.title = element_text(hjust = 0.5)) ## `geom_smooth()` using formula &#39;y ~ x&#39; 1.2.4 Tables library(xtable) ## 显示表格 knitr::kable(head(mpg)) manufacturer model displ year cyl trans drv cty hwy fl class audi a4 1.8 1999 4 auto(l5) f 18 29 p compact audi a4 1.8 1999 4 manual(m5) f 21 29 p compact audi a4 2.0 2008 4 manual(m6) f 20 31 p compact audi a4 2.0 2008 4 auto(av) f 21 30 p compact audi a4 2.8 1999 6 auto(l5) f 16 26 p compact audi a4 2.8 1999 6 manual(m5) f 18 26 p compact "],["第2次作业解答.html", "2 第2次作业解答 2.1 Question 2.2 Answer", " 2 第2次作业解答 2.1 Question 3.4, 3.11, and 3.20 2.2 Answer 2.2.1 3.4 The Rayleigh density [156, Ch. 18] is \\[ f(x)=\\frac{x}{\\sigma^{2}} e^{-x^{2} /\\left(2 \\sigma^{2}\\right)}, \\quad x \\geq 0, \\sigma&gt;0 . \\] Develop an algorithm to generate random samples from a Rayleigh \\((\\sigma)\\) distribution. Generate Rayleigh \\((\\sigma)\\) samples for several choices of \\(\\sigma&gt;0\\) and check that the mode of the generated samples is close to the theoretical mode \\(\\sigma\\) (check the histogram). 解: Rayleigh随机变量\\(X\\)的分布函数: \\[ F(x)=1-\\exp \\left(-\\frac{x^{2}}{2 \\sigma^{2}}\\right), x \\geq 0 \\] 所以\\(F^{-1}(y)=\\sigma \\sqrt{-2 \\ln (1-y)}\\) Rayleigh =function(sigma, n){ for(i in 1:n) { U=runif(n) V=1-U X = sigma * sqrt(-2 * log(V)) } return(X) } sigma = 2 n = 1000 hist(Rayleigh(sigma, n),main = &quot;Rayleigh&quot;,xlab=&quot;&quot;) 2.2.1.1 多试几组sigma sigma=c(1:9) n=1000 par(mfrow=c(3,3)) for(i in 1:9){ title=paste0(&quot;sigma=&quot;,sigma[i]) hist(Rayleigh(sigma[i], n),main = title,xlab=&quot;&quot;) } 2.2.2 3.11 Generate a random sample of size 1000 from a normal location mixture. The components of the mixture have \\(N(0,1)\\) and \\(N(3,1)\\) distributions with mixing probabilities \\(p_{1}\\) and \\(p_{2}=1-p_{1}\\). Graph the histogram of the sample with density superimposed, for \\(p_{1}=0.75 .\\) Repeat with different values for \\(p_{1}\\) and observe whether the empirical distribution of the mixture appears to be bimodal. Make a conjecture about the values of \\(p_{1}\\) that produce bimodal mixtures. 2.2.2.1 p1取0.75 n=1000 X1=rnorm(n,0,1) X2=rnorm(n,3,1) u &lt;- runif(n) p1=as.integer(u &lt; 0.75) p2=1-p1 Z=p1*X1+p2*X2 hist(Z,main = &quot;p1=0.75&quot;) 2.2.2.2 p1从0变化到1 p=seq(0,1,0.1) par(mfrow=c(3,4)) for(i in 1:11){ p1=as.integer(u &lt; p[i]) p2=1-p1 Z=p1*X1+p2*X2 title=paste0(&quot;p1=&quot;,p[i]) hist(Z,main=title) } 把\\(p1\\)的值从0变化到1，发现: 当\\(p1\\)在\\(0.5\\)附近的时候容易出现双峰. 2.2.3 3.20 A compound Poisson process is a stochastic process \\(\\{X(t), t \\geq 0\\}\\) that can be represented as the random sum \\(X(t)=\\sum_{i=1}^{N(t)} Y_{i}, t \\geq 0\\), where \\(\\{N(t), t \\geq 0\\}\\) is a Poisson process and \\(Y_{1}, Y_{2}, \\ldots\\) are id and independent of \\(\\{N(t), t \\geq 0\\}\\). Write a program to simulate a compound Poisson \\((\\lambda)\\)-Gamma process \\((Y\\) has a Gamma distribution). Estimate the mean and the variance of \\(X(10)\\) for several choices of the parameters and compare with the theoretical values. Hint: Show that \\(E[X(t)]=\\lambda t E\\left[Y_{1}\\right]\\) and \\(\\operatorname{Var}(X(t))=\\lambda t E\\left[Y_{1}^{2}\\right]\\). Poisson_Gamma=function(n, t, lambda, r, beta) { N =rpois(n, lambda * t) X=sapply(N, function(N, r, beta) sum(rgamma(N, r,beta)), r,beta) return(X) } test=function(n, t, lambda, r, beta) { x=Poisson_Gamma(n, t, lambda, r,beta) ## 样本均值 sm=mean(x) ## 理论均值 vm=var(x) ## 样本方差 tm=lambda * t * r/beta ## 理论方差 tv=lambda * t * (1 + r) * r/beta^2 ## 输出结果样式 cat(&quot;r=&quot;,r,&quot;beta=&quot;,beta,&quot;\\n&quot;) cat(&quot;样本均值:&quot;, sm, &quot; &quot;) cat(&quot;理论均值:&quot;, vm, &quot;\\n&quot;) cat(&quot;样本方差:&quot;, tm, &quot; &quot;) cat(&quot;理论方差:&quot;, tv, &quot;\\n\\n&quot;) } ## 参数值 n=1000 lambda_seq=c(1:3) r_seq=c(1:3) beta_seq=c(1:3) t=10 for (lambda in lambda_seq) { for (r in r_seq) { for (beta in beta_seq) { test(n, t, lambda,r,beta) } } } ## r= 1 beta= 1 ## 样本均值: 10.33856 理论均值: 21.19996 ## 样本方差: 10 理论方差: 20 ## ## r= 1 beta= 2 ## 样本均值: 4.975843 理论均值: 4.838821 ## 样本方差: 5 理论方差: 5 ## ## r= 1 beta= 3 ## 样本均值: 3.353112 理论均值: 2.516242 ## 样本方差: 3.333333 理论方差: 2.222222 ## ## r= 2 beta= 1 ## 样本均值: 19.71719 理论均值: 57.66402 ## 样本方差: 20 理论方差: 60 ## ## r= 2 beta= 2 ## 样本均值: 10.10393 理论均值: 16.49437 ## 样本方差: 10 理论方差: 15 ## ## r= 2 beta= 3 ## 样本均值: 6.646154 理论均值: 6.628004 ## 样本方差: 6.666667 理论方差: 6.666667 ## ## r= 3 beta= 1 ## 样本均值: 29.88445 理论均值: 118.7453 ## 样本方差: 30 理论方差: 120 ## ## r= 3 beta= 2 ## 样本均值: 14.78825 理论均值: 31.79365 ## 样本方差: 15 理论方差: 30 ## ## r= 3 beta= 3 ## 样本均值: 10.04617 理论均值: 14.5193 ## 样本方差: 10 理论方差: 13.33333 ## ## r= 1 beta= 1 ## 样本均值: 20.29056 理论均值: 39.01199 ## 样本方差: 20 理论方差: 40 ## ## r= 1 beta= 2 ## 样本均值: 10.14759 理论均值: 9.726158 ## 样本方差: 10 理论方差: 10 ## ## r= 1 beta= 3 ## 样本均值: 6.609317 理论均值: 4.428728 ## 样本方差: 6.666667 理论方差: 4.444444 ## ## r= 2 beta= 1 ## 样本均值: 39.89234 理论均值: 118.3005 ## 样本方差: 40 理论方差: 120 ## ## r= 2 beta= 2 ## 样本均值: 20.02827 理论均值: 29.06556 ## 样本方差: 20 理论方差: 30 ## ## r= 2 beta= 3 ## 样本均值: 13.30788 理论均值: 13.73979 ## 样本方差: 13.33333 理论方差: 13.33333 ## ## r= 3 beta= 1 ## 样本均值: 60.58828 理论均值: 244.0888 ## 样本方差: 60 理论方差: 240 ## ## r= 3 beta= 2 ## 样本均值: 29.92388 理论均值: 61.73559 ## 样本方差: 30 理论方差: 60 ## ## r= 3 beta= 3 ## 样本均值: 19.90115 理论均值: 24.91304 ## 样本方差: 20 理论方差: 26.66667 ## ## r= 1 beta= 1 ## 样本均值: 30.19722 理论均值: 64.34136 ## 样本方差: 30 理论方差: 60 ## ## r= 1 beta= 2 ## 样本均值: 15.04214 理论均值: 15.64235 ## 样本方差: 15 理论方差: 15 ## ## r= 1 beta= 3 ## 样本均值: 10.00053 理论均值: 6.361 ## 样本方差: 10 理论方差: 6.666667 ## ## r= 2 beta= 1 ## 样本均值: 60.29732 理论均值: 166.3114 ## 样本方差: 60 理论方差: 180 ## ## r= 2 beta= 2 ## 样本均值: 29.6664 理论均值: 41.9984 ## 样本方差: 30 理论方差: 45 ## ## r= 2 beta= 3 ## 样本均值: 19.84228 理论均值: 20.41756 ## 样本方差: 20 理论方差: 20 ## ## r= 3 beta= 1 ## 样本均值: 90.29934 理论均值: 366.9125 ## 样本方差: 90 理论方差: 360 ## ## r= 3 beta= 2 ## 样本均值: 44.9703 理论均值: 88.93843 ## 样本方差: 45 理论方差: 90 ## ## r= 3 beta= 3 ## 样本均值: 29.79438 理论均值: 39.52432 ## 样本方差: 30 理论方差: 40 "],["第3次作业解答.html", "3 第3次作业解答 3.1 Question 3.2 Answer", " 3 第3次作业解答 3.1 Question Exercises 5.4, 5.9, 5.13, and 5.14 3.2 Answer 3.2.1 5.4 Write a function to compute a Monte Carlo estimate of the Beta(3, 3) cdf, and use the function to estimate F(x) for x = 0.1, 0.2, . . ., 0.9. Compare the estimates with the values returned by the pbeta function in R. Solution: \\(Beta(3,3)=\\int_0^1 t^2(1-t)^2dt\\) MCBeta = function(x,n=10000){ t=runif(n, min = 0, max = x) theta.hat=x*mean(t*t*(1-t)*(1-t))/beta(3,3) return(theta.hat) } x=seq(0.1,0.9,0.1) MC.Beta=pbeta=rep(0,9) for(i in 1:9) { MC.Beta[i]=MCBeta(x[i]) pbeta[i]=pbeta(x[i],3,3) } out=rbind(MC.Beta,pbeta) rownames(out)=c(&quot;MC.Beta&quot;,&quot;pbeta&quot;) colname=NULL for(i in 9:1){ name=paste(&quot;x=&quot;,x[i]) colname=cbind(name,colname) } colnames(out)=colname out ## x= 0.1 x= 0.2 x= 0.3 x= 0.4 x= 0.5 x= 0.6 x= 0.7 ## MC.Beta 0.00857653 0.05822932 0.1639852 0.3169921 0.4993018 0.6828323 0.8433748 ## pbeta 0.00856000 0.05792000 0.1630800 0.3174400 0.5000000 0.6825600 0.8369200 ## x= 0.8 x= 0.9 ## MC.Beta 0.9531993 0.9921696 ## pbeta 0.9420800 0.9914400 3.2.2 5.9 The Rayleigh density \\([156,(18.76)]\\) is \\[ f(x)=\\frac{x}{\\sigma^{2}} e^{-x^{2} /\\left(2 \\sigma^{2}\\right)}, \\quad x \\geq 0, \\sigma&gt;0 \\] Implement a function to generate samples from a Rayleigh \\((\\sigma)\\) distribution, using antithetic variables. What is the percent reduction in variance of \\(\\frac{X+X^{\\prime}}{2}\\) compared with \\(\\frac{X_{1}+X_{2}}{2}\\) for independent \\(X_{1}, X_{2} ?\\) rayleigh=function(scale, n) { rayleigh=antithetic=numeric(n) for (i in 1:n) { U =runif(n) V = 1 - U rayleigh = scale * sqrt(-2 * log(U)) antithetic = scale * sqrt(-2 * log(V)) out$ray=rayleigh out$ant=antithetic } return(out) } scale=2 n=1000 out=rayleigh(scale, n) ## Warning in out$ray &lt;- rayleigh: 把公式左手强迫变成串列 var1 = var(out$ray) var2 =(var(out$ray) + var(out$ant) + 2 * cov(out$ray, out$ant)) / 4 reduction = ((var1 - var2) / var1) cat(&quot;reduction=&quot;,100*reduction,&quot;%&quot;) ## reduction= 97.51797 % 3.2.3 5.13 Find two importance functions \\(f_{1}\\) and \\(f_{2}\\) that are supported on \\((1, \\infty)\\) and are ‘close’ to \\[ g(x)=\\frac{x^{2}}{\\sqrt{2 \\pi}} e^{-x^{2} / 2}, \\quad x&gt;1 \\] Which of your two importance functions should produce the smaller variance in estimating \\[ \\int_{1}^{\\infty} \\frac{x^{2}}{\\sqrt{2 \\pi}} e^{-x^{2} / 2} d x \\] by importance sampling? Explain. Solution: Consider \\[f_1(x) = e^{-x}, x \\in (1,\\infty)\\] and \\[f_2(x) = \\frac{1}{x^2}, x \\in (1,\\infty)\\] x =seq(1,10,0.02) y = x^2/sqrt(2*pi)* exp((-x^2/2)) y1 = exp(-x) y2 =1 / (x^2) gs =c(expression(g(x)==e^{-x^2/2}*x^2/sqrt(2*pi)),expression(f[1](x)==1/(x^2)),expression(f[2](x)==x*e^{(1-x^2)/4}/sqrt(2*pi))) plot(x, y, type = &quot;l&quot;, ylab = &quot;&quot;, ylim = c(0,0.5),main=&#39;density function&#39;) lines(x, y1, lty = 2,col=&quot;red&quot;) lines(x, y2, lty = 3,col=&quot;blue&quot;) legend(&quot;topright&quot;, legend = 0:2,lty = 1:3,col=c(&quot;black&quot;,&quot;red&quot;,&quot;blue&quot;)) plot(x, y/y1,type = &quot;l&quot;,lty = 2, col=&quot;red&quot;,main = &#39;ratios&#39;) lines(x, y/y2, lty = 3,col=&quot;blue&quot;) legend(&quot;topright&quot;, legend = 1:2,lty = 2:3,col=c(&quot;red&quot;,&quot;blue&quot;)) f1 = function(x) { exp(-x) } f2 = function(x) { (pi * (1 + x^2))^(-1) * (x &gt;= 1) } g =function(x) {x^2*exp(-x^2/2)/sqrt(2*pi)*(x&gt;1)} m = 10^7 x1 = rexp(m) x2 = rcauchy(m) x2[which(x2 &lt; 1)] = 1 fg = cbind(g(x1) / f1(x1), g(x2) / f2(x2)) theta.hat = se = numeric(2) theta.hat =c(mean(fg[,1]), mean(fg[,2])) se = c(sd(fg[,1]), sd(fg[,2])) rbind(theta.hat, se) ## [,1] [,2] ## theta.hat 0.4007853 0.4003510 ## se 0.5859000 0.9585166 3.2.4 5.14 Obtain a Monte Carlo estimate of \\[ \\int_{1}^{\\infty} \\frac{x^{2}}{\\sqrt{2 \\pi}} e^{-x^{2} / 2} d x \\] by importance sampling. g =function(x) {x^2*exp(-x^2/2)/sqrt(2*pi)*(x&gt;1)} m = 1e4 u = runif(m) x = 1/(1-u) fg = g(x)*x^2 theta.hat = mean(fg) print(theta.hat) ## [1] 0.3995721 theta =integrate(g,1,Inf) theta ## 0.400626 with absolute error &lt; 5.7e-07 "],["第4次作业解答.html", "4 第4次作业解答 4.1 Question 4.2 Answer", " 4 第4次作业解答 4.1 Question Exercises 6.5 and 6.A If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, 0.651 for one method and 0.676 for another method. We want to know if the powers are different at 0.05 level. What is the corresponding hypothesis test problem? What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why? Please provide the least necessary information for hypothesis testing 4.2 Answer 4.2.1 6.5 Suppose a 95% symmetric t-interval is applied to estimate a mean, but the sample data are non-normal. Then the probability that the confidence interval covers the mean is not necessarily equal to 0.95. Use a Monte Carlo experiment to estimate the coverage probability of the t-interval for random samples of \\(χ^2(2)\\) data with sample size n = 20. Compare your t-interval results with the simulation results in Example 6.4. (The t-interval should be more robust to departures from normality than the interval for variance.) n &lt;- 20 m &lt;- 1000 alpha &lt;- .05 UCL &lt;- replicate(m,expr = { x &lt;- rchisq(n, df = 2) (n-1) * var(x) / qchisq(alpha, df = n-1) }) cat(&quot;coverage probability=&quot;, mean(UCL &gt; 4)) ## coverage probability= 0.783 the 0.95 confidence interval is\\(\\left[\\bar{X}-\\frac{S}{\\sqrt{n}} t_{n-1}(0.975), \\bar{X}+\\frac{S}{\\sqrt{n}} t_{n-1}(0.975)\\right]\\) n &lt;- 20 alpha &lt;- .05 x &lt;- rchisq(n, df = 2) prob &lt;- replicate(m,expr = { x &lt;- rchisq(n, df = 2) abs(mean(x)-2) &lt; sd(x) * qt(alpha/2, df = n-1,lower.tail = FALSE)/sqrt(n) }) cat(&quot;coverage probability=&quot;, mean(prob)) ## coverage probability= 0.921 如果样本不是正态分布的，那么置信区间覆盖方差的概率就不一定接近0.95，但t-interval表现更稳健。 4.2.2 6.A Use Monte Carlo simulation to investigate whether the empirical Type I error rate of the t-test is approximately equal to the nominal significance level\\(\\alpha\\), when the sampled population is non-normal. The t-test is robust to mild departures from normality. Discuss the simulation results for the cases where the sampled population is (i) \\(X^2(1)\\), (ii) Uniform\\((0,2)\\), and (iii) Exponential(rate=1). In each case, test \\(H_0 : \\mu= \\mu_0 ~~vs~~ H_0 : \\mu \\neq \\mu_0\\), where \\(\\mu_0\\) is the mean of \\(X^2(1)\\), Uniform\\((0,2)\\), and Exponential(1), respectively. num&lt;-c(50,100,200,500,1000) # 不同样本量 m&lt;-10000 er&lt;-NULL for (n in num){ cv&lt;-qt(0.975,n-1) er1&lt;-mean(sapply(1:m,FUN = function(o){ x&lt;-rchisq(n,1) m&lt;-mean(x) se&lt;-sqrt(var(x)) abs((m-1)*sqrt(n)/se)&gt;=cv })) er2&lt;-mean(sapply(1:m,FUN = function(o){ x&lt;-runif(n,0,2) m&lt;-mean(x) se&lt;-sqrt(var(x)) abs((m-1)*sqrt(n)/se)&gt;=cv })) er3&lt;-mean(sapply(1:m,FUN = function(o){ x&lt;-rexp(n,1) m&lt;-mean(x) se&lt;-sqrt(var(x)) abs((m-1)*sqrt(n)/se)&gt;=cv })) er&lt;-cbind(er,c(er1,er2,er3)) } colnames(er)&lt;-num rownames(er)&lt;-c(&quot;Chi(1)&quot;,&quot;U(0,2)&quot;,&quot;exp(1)&quot;) knitr::kable(er) 50 100 200 500 1000 Chi(1) 0.0755 0.0673 0.0580 0.0535 0.0530 U(0,2) 0.0529 0.0509 0.0486 0.0499 0.0532 exp(1) 0.0676 0.0594 0.0547 0.0553 0.0541 4.2.3 Discussion If we obtain the powers for two methods under a particular simulation setting with 10,000 experiments: say, \\(0.651\\) for one method and \\(0.676\\) for another method. We want to know if the powers are different at \\(0.05\\) level. What is the corresponding hypothesis test problem? What test should we use? Z-test, two-sample t-test, paired-t test or McNemar test? Why? Please provide the least necessary information for hypothesis testing. Solution. 1. \\(H_0: \\text{The two methods have the same power.} \\leftrightarrow H_1: \\text{The two methods have different powers.}\\) 2. McNemar test. Because it is equivalent to test whether the acceptance rates of the two methods are the same. Also, a contingency table can be naturally constructed as in 3. 3. For instance, consider the following contingency table. mat &lt;- matrix(c(6510, 3490, 10000, 6760, 3240, 10000, 13270, 6730, 20000), 3, 3, dimnames = list( c(&quot;Rejected&quot;, &quot;Accepted&quot;, &quot;total&quot;), c(&quot;method A&quot;, &quot;method B&quot;, &quot;total&quot;) )) mat ## method A method B total ## Rejected 6510 6760 13270 ## Accepted 3490 3240 6730 ## total 10000 10000 20000 The test statistic: \\[\\chi^2 = \\sum_{i,j=1}^2 \\frac{(n_{ij}-n_{i+} n_{+j}/n)^2}{n_{i+}n_{+j}/n} \\rightarrow \\chi^2_1.\\] Note that \\(\\chi^2 = 13.9966\\) and the p-value is \\(P(\\chi^2_1 &gt; \\chi^2) = 0.0001831415 &lt; 0.05\\). Therefore, we reject the null hypothesis \\(H_0\\), that is, the powers are different at \\(0.05\\) level. "],["第5次作业解答.html", "5 第5次作业解答 5.1 Question 5.2 Answer", " 5 第5次作业解答 5.1 Question Exercise 6.C Repeat Examples \\(6.8\\) and \\(6.10\\) for Mardia’s multivariate skewness test. Mardia [187] proposed tests of multivariate normality based on multivariate generalizations of skewness and kurtosis. If \\(X\\) and \\(Y\\) are iid, the multivariate population skewness \\(\\beta_{1, d}\\) is defined by Mardia as \\[ \\beta_{1, d}=E\\left[(X-\\mu)^{T} \\Sigma^{-1}(Y-\\mu)\\right]^{3} \\] Under normality, \\(\\beta_{1, d}=0 .\\) The multivariate skewness statistic is \\[ b_{1, d}=\\frac{1}{n^{2}} \\sum_{i, j=1}^{n}\\left(\\left(X_{i}-\\bar{X}\\right)^{T} \\widehat{\\Sigma}^{-1}\\left(X_{j}-\\bar{X}\\right)\\right)^{3} \\] where \\(\\hat{\\Sigma}\\) is the maximum likelihood estimator of covariance. Large values of \\(b_{1, d}\\) are significant. The asymptotic distribution of \\(n b_{1, d} / 6\\) is chisquared with \\(d(d+1)(d+2) / 6\\) degrees of freedom. 5.2 Answer 5.2.1 多元正态性的偏度检验(6.8) 假设为 \\[H_0:\\beta_{1,d}=0 \\leftrightarrow H_1:\\beta_{1,d}\\neq 0\\] 当多元总体为正态时，\\(\\frac{nb_{1,d}}{6}\\)的渐进分布为\\(\\chi_{d(d+1)(d+2)/6}^2\\)，对大的\\(|\\beta_{1,d}|\\)值拒绝原假设。 对大小为\\(n=10,20,30,50,100,500\\)的样本，估计基于\\(\\frac{nb_{1,d}}{6}\\)的渐进分布的多元正态性偏度检验在显著水平\\(\\alpha=0.05\\)下的第一类错误率，在正态极限分布下计算临界值向量并存储到\\(b_0\\)中。并给出mul.sk()函数用来计算样本多元偏度统计量。 nn &lt;- c(10,20,30,50,100,500) # 样本容量 alpha &lt;- 0.05 # 显著性水平 d &lt;- 2 # 随机变量的维数 b0 &lt;- qchisq(1-alpha,df=d*(d+1)*(d+2)/6)*6/nn # 每种样本容量临界值向量 # 计算多元样本偏度统计量 mul.sk &lt;- function(x){ n &lt;- nrow(x) # 样本个数 xbar &lt;- colMeans(x) sigma.hat &lt;- (n-1)/n*cov(x) # MLE估计 b &lt;- 0 for(i in 1:nrow(x)){ for(j in 1:nrow(x)){ b &lt;- b+((x[i,]-xbar)%*%solve(sigma.hat)%*%(x[j,]-xbar))^3 } } return(b/(n^2)) } # 计算第一类错误的经验估计 library(mvtnorm) set.seed(200) p.reject &lt;- vector(mode = &quot;numeric&quot;,length = length(nn)) # 保存模拟结果 m &lt;- 1000 for(i in 1:length(nn)){ mul.sktests &lt;- vector(mode = &quot;numeric&quot;,length = m) for(j in 1:m){ data &lt;- rmvnorm(nn[i],mean = rep(0,d)) mul.sktests[j] &lt;- as.integer(mul.sk(data)&gt;b0[i]) } p.reject[i] &lt;- mean(mul.sktests) } p.reject 模拟结果为第一类错误的经验估计，总结如下: summ &lt;- rbind(nn,p.reject) rownames(summ) &lt;- c(&quot;n&quot;,&quot;estimate&quot;) knitr::kable(summ) 模拟的结果说明渐进卡方分布对大小\\(n\\leq 50\\)的小样本并不合适，需要进一步求方差的精确值。 5.2.2 多元正态性偏度检验的功效(6.10) 类似例6.10，针对污染正态备择假设，通过模拟估计多元正态性偏度检验的功效，污染正态分布表示如下： \\[(1-\\epsilon)N(0,I_d)+\\epsilon N(0,100I_d),0 \\leq \\epsilon \\leq 1\\] 对一列以\\(\\epsilon\\)为指标的备择假设估计其多元偏度检验的功效，并绘制检验功效的功效函数。显著性水平\\(\\alpha=0.1\\)，样本大小为\\(n=30\\)。 alpha &lt;- 0.1 n &lt;- 30 # 样本大小 m &lt;- 2000 # 重复次数 epsilon &lt;- c(seq(0,0.15,0.01),seq(0.15,1,0.05)) N &lt;- length(epsilon) power &lt;- vector(mode = &quot;numeric&quot;,length = N) b0 &lt;- qchisq(1-alpha,df=d*(d+1)*(d+2)/6)*6/n #临界值 # 对这列epsilon分别求power for(j in 1:N){ e &lt;- epsilon[j] mul.sktests &lt;- numeric(m) for(i in 1:m){ # 生成混合分布 u &lt;- sample(c(1,0),size = n,replace = T,prob = c(1-e,e)) data1 &lt;- rmvnorm(n,sigma = diag(1,d)) data2 &lt;- rmvnorm(n,sigma = diag(100,d)) data &lt;- u*data1+(1-u)*data2 mul.sktests[i] &lt;- as.integer(mul.sk(data)&gt;b0) } power[j] &lt;- mean(mul.sktests) } # 绘制功效函数 plot(epsilon,power,type=&quot;b&quot;,xlab=bquote(epsilon),ylim=c(0,1)) abline(h=0.1,lty=3,col=&quot;lightblue&quot;) se &lt;- sqrt(power*(1-power)/m) # 绘制标准误差 lines(epsilon,power-se,lty=3) lines(epsilon,power+se,lty=3) 从图中可以看出，功效函数在两个端点\\(\\epsilon=0\\)和\\(\\epsilon=1\\)处与\\(\\alpha=0.1\\)对应的水平线相较，对于\\(0&lt;\\epsilon&lt;1\\)，经验功效函数要大于0.1，且在0.15左右达到最高。 "],["第6次作业解答.html", "6 第6次作业解答 6.1 Question 6.2 Answer", " 6 第6次作业解答 6.1 Question Exercises 7.7, 7.8, 7.9, 7.B 6.2 Answer 6.2.1 7.7, 7.8, 7.9 Refer to Exercise 7.6. Efron and Tibshirani discuss the following example [84, Ch. 7]. The five-dimensional scores data have a \\(5 \\times 5\\) covariance matrix \\(\\Sigma\\), with positive eigenvalues \\(\\lambda_{1}&gt;\\cdots&gt;\\lambda_{5}\\). In principal components analysis, \\[ \\theta=\\frac{\\lambda_{1}}{\\sum_{j=1}^{5} \\lambda_{j}} \\] measures the proportion of variance explained by the first principal component. Let \\(\\hat{\\lambda}_{1}&gt;\\cdots&gt;\\hat{\\lambda}_{5}\\) be the eigenvalues of \\(\\hat{\\Sigma}\\), where \\(\\hat{\\Sigma}\\) is the MLE of \\(\\Sigma\\). Compute the sample estimate \\[ \\hat{\\theta}=\\frac{\\hat{\\lambda}_{1}}{\\sum_{j=1}^{5} \\hat{\\lambda}_{j}} \\] of \\(\\theta\\). (7.7) Use bootstrap to estimate the bias and standard error of \\(\\hat{\\theta}\\). (7.8) Refer to Exercise 7.7. Obtain the jackknife estimates of bias and standard error of \\(\\hat{\\theta}\\) (7.9) Refer to Exercise 7.7. Compute 95% percentile and BCa confidence intervals for \\(\\hat{\\theta}\\) library(bootstrap) library(boot) set.seed(1) n &lt;- nrow(scor) B &lt;- 200 # estimate of theta lambda_hat &lt;- eigen(cov(scor))$values theta_hat &lt;- lambda_hat[1] / sum(lambda_hat) ###### bootstrap ###### theta &lt;- function(data, k) { x &lt;- data[k, ] lambda &lt;- eigen(cov(x))$values theta &lt;- lambda[1] / sum(lambda) theta } res_b &lt;- boot(data = scor, statistic = theta, R = B) theta_b &lt;- res_b$t # bias and standard error bias_b &lt;- mean(theta_b) - theta_hat se_b &lt;- sqrt(var(theta_b)) # 95% percentile and bca CIs CIs &lt;- boot.ci(res_b, conf = 0.95, type = c(&quot;perc&quot;, &quot;bca&quot;)) CI_perc &lt;- CIs$percent[4:5] CI_bca &lt;- CIs$bca[4:5] ###### Jackknife ###### theta_j &lt;- numeric(n) for (i in 1:n) { x &lt;- scor[-i, ] lambda &lt;- eigen(cov(x))$values theta_j[i] &lt;- lambda[1] / sum(lambda) } # bias and standard error bias_j &lt;- (n-1) * (mean(theta_j) - theta_hat) se_j &lt;- (n-1) * sqrt(var(theta_j) / n) # Summary res &lt;- rbind(theta_hat, bias_b, se_b, bias_j, se_j) rownames(res) &lt;- c(&quot;estimate of theta&quot;, &quot;bias_bootstrap&quot;, &quot;se_bootstrap&quot;, &quot;bias_jackknife&quot;, &quot;se_jackknife&quot;) knitr::kable(res) estimate of theta 0.6191150 bias_bootstrap 0.0016929 se_bootstrap 0.0474349 bias_jackknife 0.0010691 se_jackknife 0.0495523 cat(&quot;Percentile CI:&quot;, paste0(&quot;(&quot;,paste(CI_perc, collapse=&quot;, &quot;),&quot;)&quot;), &quot;\\n&quot;) ## Percentile CI: (0.528377009929606, 0.713197868750667) cat(&quot;BCa CI:&quot;, paste0(&quot;(&quot;,paste(CI_bca, collapse=&quot;, &quot;),&quot;)&quot;), &quot;\\n&quot;) ## BCa CI: (0.518723530142406, 0.712955398088904) 6.2.2 7.B Repeat Project 7.A for the sample skewness statistic. Compare the coverage rates for normal populations (skewness 0) and \\(χ^2(5)\\) distributions (positive skewness) The true skewness of \\(N(0,1)\\) and \\(\\chi^2(5)\\) are \\(0\\) and \\(\\sqrt{8/5}\\), respectively. # skewness statistic sk &lt;- function(data, k) { x &lt;- data[k] sum((x-mean(x))^3)/((length(x)-1)*sd(x)^3) } # true skewness sk_norm &lt;- 0 sk_chisq &lt;- sqrt(8/5) sim_once &lt;- function(seed) { # generate data set.seed(seed) x_norm &lt;- rnorm(100) x_chisq &lt;- rchisq(100, df = 5) ##### bootstrap ######## B &lt;- 200 res_norm &lt;- boot(data = x_norm, statistic = sk, R = B) res_chisq &lt;- boot(data = x_chisq, statistic = sk, R = B) # CIs CI_norm &lt;- boot.ci(res_norm, conf = 0.95, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;)) CI_chisq &lt;- boot.ci(res_chisq, conf = 0.95, type = c(&quot;norm&quot;, &quot;basic&quot;, &quot;perc&quot;)) # coverage fl_norm_norm &lt;- (sk_norm &gt;= CI_norm$percent[4]) &amp; (sk_norm &lt;= CI_norm$percent[5]) fl_norm_basic &lt;- (sk_norm &gt;= CI_norm$basic[4]) &amp; (sk_norm &lt;= CI_norm$basic[5]) fl_norm_perc &lt;- (sk_norm &gt;= CI_norm$perc[4]) &amp; (sk_norm &lt;= CI_norm$perc[5]) fl_chisq_norm &lt;- (sk_chisq &gt;= CI_chisq$percent[4]) &amp; (sk_chisq &lt;= CI_chisq$percent[5]) fl_chisq_basic &lt;- (sk_chisq &gt;= CI_chisq$basic[4]) &amp; (sk_chisq &lt;= CI_chisq$basic[5]) fl_chisq_perc &lt;- (sk_chisq &gt;= CI_chisq$perc[4]) &amp; (sk_chisq &lt;= CI_chisq$perc[5]) return(c(fl_norm_norm = fl_norm_norm, fl_norm_basic = fl_norm_basic, fl_norm_perc = fl_norm_perc, fl_chisq_norm = fl_chisq_norm, fl_chisq_basic = fl_chisq_basic, fl_chisq_perc = fl_chisq_perc)) } M &lt;- 200 # replicates res &lt;- sapply(1:M, sim_once) res_mean &lt;- as.data.frame(rowMeans(res)) colnames(res_mean) &lt;- NULL rownames(res_mean) &lt;- c(&quot;N(0,1)_normal&quot;, &quot;N(0,1)_basic&quot;, &quot;N(0,1)_perc&quot;, &quot;Chisq(5)_norm&quot;, &quot;Chisq(5)_basic&quot;, &quot;Chisq(5)_perc&quot;) cat(&quot;coverage rate&quot;, &quot;\\n&quot;) ## coverage rate knitr::kable(res_mean) N(0,1)_normal 0.905 N(0,1)_basic 0.880 N(0,1)_perc 0.905 Chisq(5)_norm 0.765 Chisq(5)_basic 0.735 Chisq(5)_perc 0.765 The table above shows the coverage rates respectively. For normal distribution, the empirical levels of bootstrap CIs are close to the nominal level (\\(0.95\\)), while for Chi square distribution, the empirical levels are much lower. "],["第7次作业解答.html", "7 第7次作业解答 7.1 Question 7.2 Answer 7.3 Unequal variances and unequal expectations", " 7 第7次作业解答 7.1 Question Exercise 8.2 (page 242, Statistical Computating with R). Design experiments for evaluating the performance of the NN, energy, and ball methods in various situations. Unequal variances and equal expectations Unequal variances and unequal expectations Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions) Unbalanced samples (say, 1 case versus 10 controls) Note: The parameters should be chosen such that the powers are distinguishable (say, range from 0.3 to 0.8). 7.2 Answer 7.2.1 8.2 Implement the bivariate Spearman rank correlation test for independence [255] as a permutation test. The Spearman rank correlation test statistic can be obtained from function cor with method = “spearman”. Compare the achieved significance level of the permutation test with the p-value reported by cor.test on the same samples. 7.2.1.1 Spearman rank correlation test Spearman_rank_test &lt;- function(x, y, B = 1e4){ t0 = cor(x,y,method = &quot;spearman&quot;) perm = numeric(B) z = c(x,y) for(i in 1:B){ samp = sample(z) perm[i] = cor(samp[1:length(x)], samp[(length(x)+1):length(z)], method = &quot;spearman&quot;) } p_value = mean(abs(perm)&gt;=abs(t0)) return(list(statistic = t0, &#39;p.value&#39; = p_value)) } 7.2.1.2 Independent case We generate 50 samples from \\(N(0,1)\\) for \\(X\\) and \\(Y\\), respectively. We expect the p-value of the test is very large so that the null hypothesis would not be rejected. n = 50 x = rnorm(n, 0, 1) y = rnorm(n, 0, 1) Spearman_rank_test(x, y) ## $statistic ## [1] 0.1014646 ## ## $p.value ## [1] 0.4943 cor.test(x, y, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: x and y ## S = 18712, p-value = 0.4821 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.1014646 From the output of Spearman_rank_test() and cor.test, we can see the p-value of these two methods very close and both of them will not reject the null hypothesis, which mean two samples are independent. 7.2.1.3 Dependent case Consider the data \\(X\\sim N(0,1)\\), \\(Y_i = X_i + \\varepsilon_i\\), where \\(\\varepsilon_i\\sim N(0,1)\\). \\(X\\) and \\(Y\\) are highly dependent, we expect the p-value of the test is very small. n = 50 x = rnorm(n, 0, 1) y = x + rnorm(n, 0, 1) Spearman_rank_test(x, y) ## $statistic ## [1] 0.7369508 ## ## $p.value ## [1] 0 cor.test(x, y, method = &quot;spearman&quot;) ## ## Spearman&#39;s rank correlation rho ## ## data: x and y ## S = 5478, p-value = 3.121e-09 ## alternative hypothesis: true rho is not equal to 0 ## sample estimates: ## rho ## 0.7369508 From the output of Spearman_rank_test() and cor.test, we can see p-value of two methods are 0, which means we would reject the null hypothesis, i.e. two samples are not independent. 7.2.2 Discussion Design experiments for evaluating the performance of the NN, energy, and ball methods in various situations. Unequal variances and equal expectations Unequal variances and unequal expectations Non-normal distributions: t distribution with 1 df (heavy-tailed distribution), bimodel distribution (mixture of two normal distributions) Unbalanced samples (say, 1 case versus 10 controls) Note: The parameters should be chosen such that the powers are distinguishable (say, range from 0.3 to 0.8). 7.2.2.1 Unequal variances and equal expectations library(bootstrap) library(boot) library(RANN) library(energy) library(Ball) Tn &lt;- function(z, ix, size, k = 3){ n1 = size[1] n2 = size[2] n = n1+n2 if(is.vector(z)) z = data.frame(z,0) z = z[ix,] NN = nn2(data=z, k = k+1) b1 = NN$nn.idx[1:n1,-1] b2 = NN$nn.idx[(n1+1):n,-1] i1 = sum(b1&lt;n1+0.5) i2 = sum(b2&gt;n1+0.5) (i1+i2)/(k+n) } eqdist.nn &lt;- function(z, size, k, R = R){ res = boot(data = z, statistic = Tn, R = R, sim = &quot;permutation&quot;, size = size, k = k) stat = c(res$t0, res$t) p_val = mean(stat&gt;stat[1]) return(p_val) } p_val &lt;- function(x,y, R = 999, k = 3){ x = as.matrix(x) y = as.matrix(y) n1 = nrow(x) n2 = nrow(y) N = c(n1,n2) z = rbind(x,y) p_nn = eqdist.nn(z, size = N, k, R = R) p_energy = eqdist.etest(z,sizes=N,R = R)$p.value p_ball = bd.test(x=x,y=y,num.permutations = R)$p.value names(p_ball) = &quot;ball&quot; return(c(NN = p_nn, energy = p_energy, p_ball)) } 7.2.2.2 Unequal variances and equal expectations Generate \\(X_1,\\cdots,X_{50}\\) from \\(N(0,1)\\), \\(Y_1,\\cdots,Y_{50}\\) from \\(N(0,1.7^2)\\), \\(X\\) and \\(Y\\) have equal mean and unequal variance. res = replicate(200, expr = { x = rnorm(50) y = rnorm(50, 0, 1.7) p_val(x, y) }) alpha = 0.05 apply(res, 1, function(x)mean(x&lt;alpha)) ## NN energy ball ## 0.390 0.455 0.690 7.3 Unequal variances and unequal expectations Generate \\(X_1,\\cdots,X_{40}\\) from \\(N(1,1)\\), \\(Y_1,\\cdots,Y_{40}\\) from \\(N(0.6,2^2)\\), \\(X\\) and \\(Y\\) have unequal mean and unequal variance. res = replicate(200, expr = { x = rnorm(40, 1, 1) y = rnorm(40, 0.6, 2) p_val(x, y) }) alpha = 0.05 apply(res, 1, function(x)mean(x&lt;alpha)) ## NN energy ball ## 0.560 0.705 0.850 7.3.0.1 Non-normal distributions Generate \\(X_1,\\cdots, X_{40}\\) from unifrom distribution U(-3,3), generate \\(Y_1,\\cdots, Y_{60}\\) from res = replicate(200, expr = { x = runif(40, -3, 3) y = rt(60, 5) p_val(x, y) }) alpha = 0.05 apply(res, 1, function(x) mean(x&lt;alpha)) ## NN energy ball ## 0.530 0.590 0.825 7.3.0.2 Unbalanced samples res = replicate(200, expr = { x = runif(30, -3, 3) y = rt(300, 5) p_val(x, y) }) alpha = 0.05 apply(res, 1, function(x) mean(x&lt;alpha)) ## NN energy ball ## 0.405 0.775 0.870 "],["第8次作业解答.html", "8 第8次作业解答 8.1 Question 8.2 Answer", " 8 第8次作业解答 8.1 Question Exercies 9.3 and 9.8 (pages 277-278, Statistical Computating with R). For each of the above exercise, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to $R&lt; 1.2 $ 8.2 Answer 8.2.1 9.3 Use the Metropolis-Hastings sampler to generate random variables from a standard Cauchy distribution. Discard the first 1000 of the chain, and compare the deciles of the generated observations with the deciles of the standard Cauchy distribution (see qcauchy or qt with df=1). Recall that a Cauchy \\((\\theta, \\eta)\\) distribution has density function \\[ f(x)=\\frac{1}{\\theta \\pi\\left(1+[(x-\\eta) / \\theta]^{2}\\right)}, \\quad-\\infty&lt;x&lt;\\infty, \\theta&gt;0 \\] The standard Cauchy has the Cauchy \\((\\theta=1, \\eta=0)\\) density. (Note that the standard Cauchy density is equal to the Student \\(\\mathrm{t}\\) density with one degree of freedom.) set.seed(123) ## standard Cauchy f &lt;- function(x) { return(1/(pi*(1+x^2))) } m &lt;- 10000 x &lt;- numeric(m) x[1] &lt;- rnorm(1) k &lt;- 0 u &lt;- runif(m) for (i in 2:m) { xt &lt;- x[i-1] y &lt;- rnorm(1, mean = xt) num &lt;- f(y) * dnorm(xt, mean = y) den &lt;- f(xt) * dnorm(y, mean = xt) if (u[i] &lt;= num/den) x[i] &lt;- y else { x[i] &lt;- xt k &lt;- k+1 #y is rejected } } ## MC过程图 plot(1:m, x, type=&quot;l&quot;, main=&quot;&quot;, ylab=&quot;x&quot;) b &lt;- 1000 y &lt;- x[b:m] a &lt;- ppoints(1000) Qc &lt;- qcauchy(a) Q &lt;- quantile(x, a) ## QQ图 qqplot(Qc, Q, main=&quot;&quot;,xlim=c(-5,5),ylim=c(-5,5),xlab=&quot;Standard Cauchy Quantiles&quot;, ylab=&quot;Sample Quantiles&quot;) ## 直方图 hist(y, breaks=&quot;scott&quot;, main=&quot;&quot;, xlab=&quot;&quot;, freq=FALSE) lines(Qc, f(Qc)) 8.2.2 9.8 This example appears in [40]. Consider the bivariate density \\[ f(x, y) \\propto\\left(\\begin{array}{l} n \\\\ x \\end{array}\\right) y^{x+a-1}(1-y)^{n-x+b-1}, \\quad x=0,1, \\ldots, n, 0 \\leq y \\leq 1 \\] It can be shown (see e.g. \\([23]\\) ) that for fixed \\(a, b, n\\), the conditional distributions are \\(\\operatorname{Binomial}(n, y)\\) and \\(\\operatorname{Beta}(x+a, n-x+b)\\). Use the Gibbs sampler to generate a chain with target joint density \\(f(x, y)\\). set.seed(2) N &lt;- 5000 #length of chain burn &lt;- 1000 #burn-in length a &lt;- 2 b &lt;- 3 n &lt;- 1000 X &lt;- matrix(0, N, 2) #the chain, a bivariate sample #initialize x0=2 y0=0.5 ###### generate the chain ##### X[1, ] &lt;- c(x0,y0) #initialize for (i in 2:N) { y &lt;- X[i-1, 2] X[i, 1] &lt;- rbinom(1,size=n,prob = y) x &lt;- X[i, 1] X[i, 2] &lt;- rbeta(1,x+a,n-x+b) } lab &lt;- burn + 1 out &lt;- X[lab:N, ] plot(X[,1],X[,2],xlab = &quot;x&quot;,ylab = &quot;y&quot;) cat(&quot;协方差矩阵\\n&quot;) ## 协方差矩阵 cov(out) ## 协方差矩阵 ## [,1] [,2] ## [1,] 30062.95389 29.91243495 ## [2,] 29.91243 0.02994719 cat(&quot;相关系数矩阵\\n&quot;) ## 相关系数矩阵 cor(out) ## 相关系数矩阵 ## [,1] [,2] ## [1,] 1.0000000 0.9969145 ## [2,] 0.9969145 1.0000000 plot(out, main=&quot;&quot;, cex=.5, xlab=bquote(X[1]), ylab=bquote(X[2]), ylim=range(out[,2])) ## 分布图 8.2.3 Discussion For each of the above exercise, use the Gelman-Rubin method to monitor convergence of the chain, and run the chain until it converges approximately to the target distribution according to \\(\\hat R&lt;1.2\\) 解答： # 计算Gelman-Rubin statistic的函数 Gelman.Rubin &lt;- function(psi) { # psi[i,j] is the statistic psi(X[i,1:j]) # for chain in i-th row of X psi &lt;- as.matrix(psi) n &lt;- ncol(psi) k &lt;- nrow(psi) psi.means &lt;- rowMeans(psi) #row means B &lt;- n * var(psi.means) #between variance est. psi.w &lt;- apply(psi, 1, &quot;var&quot;) #within variances W &lt;- mean(psi.w) #within est. v.hat &lt;- W*(n-1)/n + (B/n) #upper variance est. r.hat &lt;- v.hat / W #G-R statistic return(r.hat) } 8.2.3.1 9.3 先按照9.3题写出构造标准柯西分布的Metropolis chain的函数 # 生成标准柯西分布的Metropolis chain # 提议函数仍取9.3中使用的对称正态分布 N(0,X[t]^2) # X1为初始值 Standard_Cauchy_Chain &lt;- function(N, X1){ X &lt;- numeric(N) X[1] &lt;- X1 #初始值 for(i in 2:N){ Xt &lt;- X[i-1] Y &lt;- rnorm(1,0,abs(Xt)) r &lt;- dt(Y,1)*dnorm(Xt,0,abs(Y))/dt(Xt,1)/dnorm(Y,0,abs(Xt)) U &lt;- runif(1) if(r &gt; 1) r &lt;- 1 if(U &lt;= r) X[i] &lt;- Y else X[i] &lt;- Xt } return(X) } 接下来不妨考虑生成4条上述Metropolis chain，每条样本量N=8000。 k &lt;- 4 N &lt;- 8000 b &lt;- 1000 #burn-in length X1 &lt;- c(0.1,0.2,0.1,0.2) #初始值 # 生成4条样本 set.seed(12345) X &lt;- matrix(0, nrow = k, ncol = N) for(i in 1:k){ X[i,] &lt;- Standard_Cauchy_Chain(N, X1[i]) } # compute diagnostic statistics psi &lt;- t(apply(X, 1, cumsum)) for (i in 1:nrow(psi)) psi[i,] &lt;- psi[i,] / (1:ncol(psi)) print(Gelman.Rubin(psi)) ## [1] 1.127139 # 四条样本的psi for (i in 1:k) if(i==1){ plot((b+1):N,psi[i, (b+1):N],ylim=c(-1,1), type=&quot;l&quot;, xlab=&#39;Index&#39;, ylab=bquote(phi)) }else{ lines(psi[i, (b+1):N], col=i) } par(mfrow=c(1,1)) 实际上发现四条样本的psi图并没有呈现逼近同一分布的结果，这可能是因为Cauchy分布的期望和方差不均存在，进而导致的估计不稳定性，下面再画出\\(\\hat R\\)统计量v.s.样本量N的图。 par(mfrow=c(1,1)) #plot the sequence of R-hat statistics rhat &lt;- rep(0, N) for (j in (b+1):N) rhat[j] &lt;- Gelman.Rubin(psi[,1:j]) plot(rhat[(b+1):N], type=&quot;l&quot;, xlab=&quot;&quot;, ylab=&quot;R&quot;) abline(h=1.2, lty=2) \\(\\hat R\\) 大概在样本为1000时达到收敛。 8.2.3.2 9.8 先按照9.8题写出题中二元随机变量的Gibbs sampler，这里不妨取a=b=1。 # 生成二元随机变量的Gibbs sampler # X1为初始值 Bivariate.Gibbs &lt;- function(N, X1){ a &lt;- b &lt;- 1 X &lt;- matrix(0, N, 2) X[1,] &lt;- X1 #初始值 for(i in 2:N){ X2 &lt;- X[i-1, 2] X[i,1] &lt;- rbinom(1,25,X2) X1 &lt;- X[i,1] X[i,2] &lt;- rbeta(1,X1+a,25-X1+b) } return(X) } 不妨还是考虑生成4条样本，每条样本量N=8000. k &lt;- 4 N &lt;- 8000 b &lt;- 1000 #burn-in length X1 &lt;- cbind(c(2,7,10,15),runif(4)) #初始值 #生成4条样本，每个第一维的放在X中，第二维的放在Y中 set.seed(12345) X &lt;- matrix(0, nrow=k, ncol=N) Y &lt;- matrix(0, nrow=k, ncol=N) for (i in 1:k){ BG &lt;- Bivariate.Gibbs(N, X1[i,]) X[i, ] &lt;- BG[,1] Y[i, ] &lt;- BG[,2] } 下面分别在每一个维度上考虑利用Gelman-Rubin method考虑样本的收敛情况。 # 先考虑第一维样本X #compute diagnostic statistics psi &lt;- t(apply(X, 1, cumsum)) for (i in 1:nrow(psi)) psi[i,] &lt;- psi[i,] / (1:ncol(psi)) #plot the sequence of R-hat statistics rhat &lt;- rep(0, N) for (j in (b+1):N) rhat[j] &lt;- Gelman.Rubin(psi[,1:j]) plot(rhat[(b+1):N], type=&quot;l&quot;, xlab=&quot;&quot;, ylab=&quot;R&quot;) abline(h=1.2, lty=2) # 再考虑第二维样本Y #compute diagnostic statistics psi &lt;- t(apply(Y, 1, cumsum)) for (i in 1:nrow(psi)) psi[i,] &lt;- psi[i,] / (1:ncol(psi)) #plot the sequence of R-hat statistics rhat &lt;- rep(0, N) for (j in (b+1):N) rhat[j] &lt;- Gelman.Rubin(psi[,1:j]) plot(rhat[(b+1):N], type=&quot;l&quot;, xlab=&quot;&quot;, ylab=&quot;R&quot;) abline(h=1.2, lty=2) 综合考虑两个维度的\\(\\hat R\\)统计量，大约在样本为4000时可以达到收敛。 "],["第9次作业解答.html", "9 第9次作业解答 9.1 Question 9.2 Answer", " 9 第9次作业解答 9.1 Question Exercises 11.3 and 11.5 (pages 353-354, Statistical Computing with R). Suppose \\(T_{1}, \\ldots, T_{n}\\) are i.i.d. samples drawn from the exponential distribution with expectation \\(\\lambda\\). Those values greater than \\(\\tau\\) are not observed due to right censorship, so that the observed values are \\(Y_{i}=T_{i} I\\left(T_{i} \\leq \\tau\\right)+\\tau I\\left(T_{i}&gt;\\tau\\right), i=1, \\ldots, n\\). Suppose \\(\\tau=1\\) and the observed \\(Y_{i}\\) values are as follows:\\[0.54,0.48,0.33,0.43,1.00,1.00,0.91,1.00,0.21,0.85\\] Use the E-M algorithm to estimate \\(\\lambda\\), compare your result with the observed data MLE (note: \\(Y_{i}\\) follows a mixture distribution). 9.2 Answer 9.2.1 11.3 Write a function to compute the \\(k^{t h}\\) term in \\[ \\sum_{k=0}^{\\infty} \\frac{(-1)^{k}}{k ! 2^{k}} \\frac{\\|a\\|^{2 k+2}}{(2 k+1)(2 k+2)} \\frac{\\Gamma\\left(\\frac{d+1}{2}\\right) \\Gamma\\left(k+\\frac{3}{2}\\right)}{\\Gamma\\left(k+\\frac{d}{2}+1\\right)} \\] where \\(d \\geq 1\\) is an integer, \\(a\\) is a vector in \\(\\mathbb{R}^{d}\\), and \\(\\|\\cdot\\|\\) denotes the Euclidean norm. Perform the arithmetic so that the coefficients can be computed for (almost) arbitrarily large \\(k\\) and \\(d\\). (This sum converges for all \\(a \\in \\mathbb{R}^{d}\\) ). Modify the function so that it computes and returns the sum. Evaluate the sum when \\(a=(1,2)^{T}\\). 解: thek &lt;- function(k, a, d){ (-1)^k/exp(lgamma(k+1)+k*log(2)) * exp((k+1)*log(sum(a^2))-log(2*k+1)-log(2*k+2)) * exp(lgamma((d+1)/2)+lgamma(k+1.5)-lgamma(k+d/2+1))#用到了gamma函数和阶乘的恒等式 } sumk &lt;- function(a, d){ k &lt;- 0 s &lt;- 0 while(abs(thek(k, a, d))&gt;1e-5){#tolerance s &lt;- s+thek(k, a, d) k &lt;- k+1 } return(s) } a &lt;- c(1,2) d &lt;- length(a) s &lt;- sumk(a,d) paste(&quot;The sum =&quot;, s) ## [1] &quot;The sum = 1.53215765372619&quot; 9.2.2 11.5 Write a function to solve the equation \\[ \\begin{gathered} \\frac{2 \\Gamma\\left(\\frac{k}{2}\\right)}{\\sqrt{\\pi(k-1)} \\Gamma\\left(\\frac{k-1}{2}\\right)} \\int_{0}^{c_{k-1}}\\left(1+\\frac{u^{2}}{k-1}\\right)^{-k / 2} d u \\\\ =\\frac{2 \\Gamma\\left(\\frac{k+1}{2}\\right)}{\\sqrt{\\pi k} \\Gamma\\left(\\frac{k}{2}\\right)} \\int_{0}^{c_{k}}\\left(1+\\frac{u^{2}}{k}\\right)^{-(k+1) / 2} d u \\end{gathered} \\] for \\(a\\), where \\[ c_{k}=\\sqrt{\\frac{a^{2} k}{k+1-a^{2}}} \\] Compare the solutions with the points \\(A(k)\\) in Exercise 11.4. 解: k &lt;- c(4:25, 100, 500, 1000) ###11.5 beijif &lt;- function(u, kf){ (1+u^2/kf)^(-(kf+1)/2) } g &lt;- function(a, kg){ ckl &lt;- sqrt(a^2*(kg-1)/(kg-a^2)) LHS &lt;- 2/sqrt(pi*(kg-1)) * exp(lgamma(kg/2)-lgamma((kg-1)/2)) * integrate(beijif, lower = 0, upper = ckl, kf=kg-1)$value ckr &lt;- sqrt(a^2*kg/(kg+1-a^2)) RHS &lt;-2/sqrt(pi*kg) * exp(lgamma((kg+1)/2)-lgamma(kg/2)) * integrate(beijif, lower = 0, upper = ckr, kf=kg)$value LHS-RHS } solution5 &lt;- numeric(length(k)) for (i in 1:length(k)) { solution5[i] &lt;- uniroot(g, c(1,2), kg=k[i])$root } ###11.4 h &lt;- function (a,kh) { (1-pt(sqrt(a^2*(kh-1) / (kh-a^2)), df=kh-1)) - (1-pt(sqrt(a^2*kh / (kh+1-a^2)), df=kh)) } solution4 &lt;- numeric(length(k)) for (i in 1:length(k)) { solution4[i] &lt;- uniroot(h, c(1,2), kh=k[i])$root } ###Compare print(cbind(k=k, exercice4=solution4, exercice4=solution5)) ## k exercice4 exercice4 ## [1,] 4 1.492103 1.492103 ## [2,] 5 1.533556 1.533556 ## [3,] 6 1.562744 1.562744 ## [4,] 7 1.584430 1.584430 ## [5,] 8 1.601185 1.601185 ## [6,] 9 1.614521 1.614521 ## [7,] 10 1.625390 1.625390 ## [8,] 11 1.634419 1.634419 ## [9,] 12 1.642038 1.642038 ## [10,] 13 1.648554 1.648554 ## [11,] 14 1.654190 1.654190 ## [12,] 15 1.659114 1.659114 ## [13,] 16 1.663452 1.663452 ## [14,] 17 1.667303 1.667303 ## [15,] 18 1.670745 1.670745 ## [16,] 19 1.673840 1.673840 ## [17,] 20 1.676637 1.676637 ## [18,] 21 1.679178 1.679178 ## [19,] 22 1.681496 1.681496 ## [20,] 23 1.683620 1.683620 ## [21,] 24 1.685572 1.685572 ## [22,] 25 1.687373 1.687373 ## [23,] 100 1.720608 1.720608 ## [24,] 500 1.729755 1.729755 ## [25,] 1000 1.730907 1.730907 两种方法结果完全一致。 9.2.3 Discussion Suppose \\(T_{1}, \\ldots, T_{n}\\) are i.i.d. samples drawn from the exponential distribution with expectation \\(\\lambda\\). Those values greater than \\(\\tau\\) are not observed due to right censorship, so that the observed values are \\(Y_{i}=T_{i} I\\left(T_{i} \\leq \\tau\\right)+\\tau I\\left(T_{i}&gt;\\tau\\right), i=1, \\ldots, n\\). Suppose \\(\\tau=1\\) and the observed \\(Y_{i}\\) values are as follows:\\[0.54,0.48,0.33,0.43,1.00,1.00,0.91,1.00,0.21,0.85\\] Use the E-M algorithm to estimate \\(\\lambda\\), compare your result with the observed data MLE (note: \\(Y_{i}\\) follows a mixture distribution). 解: Observed data likelihood: \\[L=\\Pi_{i=1}^n\\left(\\frac{1}{\\lambda} e^{-\\frac{1}{\\lambda} x_{i}}\\right)^{k_{i}} \\cdot\\left(e^{-\\frac{1}{\\lambda} \\tau}\\right)^{1-k_{i}}\\] \\[\\log L=n_{1} \\cdot \\log \\frac{1}{\\lambda}+\\sum_{x_{i} \\leq \\tau}\\left(-\\frac{1}{\\lambda} x_{i}\\right)+n_{2} \\cdot-\\frac{1}{\\lambda} \\tau\\] \\[\\frac{\\partial \\log L}{\\partial\\lambda}=n_{1} \\cdot \\left(-\\frac{1}{\\lambda}\\right)+ \\left(\\sum_{x_{i} \\leq \\tau} \\frac{1}{\\lambda^2}x_{i}\\right)+n_{2}\\cdot\\frac{1}{\\lambda^2} \\tau\\] \\[\\hat{\\lambda}_{MLE}=\\frac{\\sum_{x_{i} \\leq \\tau} x_{i}+n_{2} \\tau}{n_{1}}\\] Complete data likelihood: \\[L=\\Pi_{i=1}^n \\frac{1}{\\lambda} e^{-\\frac{1}{\\lambda} x_{i}}\\] \\[\\log L= n \\cdot \\log \\frac{1}{\\lambda}-\\frac{1}{\\lambda} \\cdot \\Sigma_{x_{i} \\leq \\tau} x_{i}-\\frac{1}{\\lambda} \\Sigma_{x_{i} &gt; \\tau} x_{i}\\] E-step: \\[\\mathbb{E}\\log L=n \\cdot \\log \\frac{1}{\\lambda}-\\frac{1}{\\lambda} \\cdot \\Sigma_{x_{i} \\leq \\tau} x_{i}- \\frac{1}{\\lambda}n_{2}\\left(\\tau+\\lambda_{0}\\right)\\] \\[\\frac{\\partial \\mathbb{E} \\log L}{\\partial \\lambda}=n\\cdot \\left(-\\frac{1}{\\lambda}\\right)+\\frac{1}{\\lambda^2}\\cdot\\Sigma_{x_{i} \\leq \\tau} x_{i}+\\frac{1}{\\lambda^2}\\cdot n_{2}\\left(\\tau+\\lambda_{0}\\right)=0\\] M-step: \\[\\lambda_1=\\frac{\\Sigma_{x_{i} \\leq \\tau} x_{i}+n_{2}\\left(\\tau+\\lambda_{0}\\right)}{n}\\] \\[\\hat{\\lambda}_{EM}=\\frac{\\sum_{x_{i} \\leq \\tau} x_{i}+n_{2} \\tau}{n_{1}}\\] data &lt;- c(0.54,0.48,0.33,0.43,1.00,1.00,0.91,1.00,0.21,0.85) tau &lt;- 1 n &lt;- length(data) n1 &lt;- sum(data&lt;tau) n2 &lt;- n-n1 lam0 &lt;- 0 lam1 &lt;- 1#初始值 i &lt;- 1 while (abs(lam0-lam1)&gt;1e-10) { lam0 &lt;- lam1 # E step E &lt;- function(lam) n*log(1/lam)-1/lam*sum(data[data&lt;tau])-n2/lam*(tau+lam0) # M step lam1 &lt;- optimize(E, lower = 0, upper = 2, maximum = TRUE)$maximum } # MLE # lam &lt;- 1 lik &lt;- function(lam){ lik1 &lt;- sapply(data[data&lt;tau], function(x) { dexp(x,rate=1/lam) }) lik2 &lt;- sapply(data[data==tau],function(x){ 1-pexp(tau,rate = 1/lam) }) prod(c(lik1,lik2)) } MLE &lt;- optimize(lik, lower = 0, upper = 2, maximum = TRUE)$maximum print(cbind(EM=lam1, MLE)) ## EM MLE ## [1,] 0.9642917 0.9643037 结果相差很小. "],["第10次作业解答.html", "10 第10次作业解答 10.1 Question 10.2 Answer", " 10 第10次作业解答 10.1 Question Exercises 1 and 5 (page 204, Advanced R) Excecises 1 and 7 (page 214, Advanced R) 10.2 Answer 10.2.1 P204-1 Why are the following two invocations of lapply() equivalent? trims &lt;- c(0, 0.1, 0.2, 0.5) x &lt;- rcauchy(100) lapply(trims, function(trim) mean(x, trim = trim)) lapply(trims, mean, x = x) 答: mean(x,trim)实际上有两个参数x和trim 第一种表达是构造了新的function,给定了x的值，只剩下一个参数trim, lapply可以直接对trim的每个取值调用新的函数求mean 第二种表达实际上把mean的参数x直接传到lapply里了, 也是对trim每个取值求mean 10.2.2 P204-5 For each model in the previous two exercises, extract R2 using the function below. 10.2.2.1 Ex1 formulas &lt;- list( mpg ~ disp, mpg ~ I(1 / disp), mpg ~ disp + wt, mpg ~ I(1 / disp) + wt ) lapply(formulas,lm,data=mtcars) ## [[1]] ## ## Call: ## FUN(formula = X[[i]], data = ..1) ## ## Coefficients: ## (Intercept) disp ## 29.59985 -0.04122 ## ## ## [[2]] ## ## Call: ## FUN(formula = X[[i]], data = ..1) ## ## Coefficients: ## (Intercept) I(1/disp) ## 10.75 1557.67 ## ## ## [[3]] ## ## Call: ## FUN(formula = X[[i]], data = ..1) ## ## Coefficients: ## (Intercept) disp wt ## 34.96055 -0.01772 -3.35083 ## ## ## [[4]] ## ## Call: ## FUN(formula = X[[i]], data = ..1) ## ## Coefficients: ## (Intercept) I(1/disp) wt ## 19.024 1142.560 -1.798 10.2.2.2 Ex2 bootstraps &lt;- lapply(1:10, function(i) { rows &lt;- sample(1:nrow(mtcars), rep = TRUE) mtcars[rows, ] }) lapply(bootstraps,function(t) lm(mpg~disp,data=t)) ## [[1]] ## ## Call: ## lm(formula = mpg ~ disp, data = t) ## ## Coefficients: ## (Intercept) disp ## 30.07649 -0.04254 ## ## ## [[2]] ## ## Call: ## lm(formula = mpg ~ disp, data = t) ## ## Coefficients: ## (Intercept) disp ## 29.01307 -0.03946 ## ## ## [[3]] ## ## Call: ## lm(formula = mpg ~ disp, data = t) ## ## Coefficients: ## (Intercept) disp ## 29.57283 -0.03867 ## ## ## [[4]] ## ## Call: ## lm(formula = mpg ~ disp, data = t) ## ## Coefficients: ## (Intercept) disp ## 29.77616 -0.04194 ## ## ## [[5]] ## ## Call: ## lm(formula = mpg ~ disp, data = t) ## ## Coefficients: ## (Intercept) disp ## 30.37967 -0.04526 ## ## ## [[6]] ## ## Call: ## lm(formula = mpg ~ disp, data = t) ## ## Coefficients: ## (Intercept) disp ## 28.20938 -0.03726 ## ## ## [[7]] ## ## Call: ## lm(formula = mpg ~ disp, data = t) ## ## Coefficients: ## (Intercept) disp ## 31.01170 -0.04959 ## ## ## [[8]] ## ## Call: ## lm(formula = mpg ~ disp, data = t) ## ## Coefficients: ## (Intercept) disp ## 30.86442 -0.04449 ## ## ## [[9]] ## ## Call: ## lm(formula = mpg ~ disp, data = t) ## ## Coefficients: ## (Intercept) disp ## 30.04473 -0.04182 ## ## ## [[10]] ## ## Call: ## lm(formula = mpg ~ disp, data = t) ## ## Coefficients: ## (Intercept) disp ## 28.54571 -0.03534 rsq &lt;- function(mod) summary(mod)$r.squared ## Ex1 cat(&quot;这是Ex1 4个模型的R-square\\n&quot;) ## 这是Ex1 4个模型的R-square lapply(lapply(formulas,lm,data=mtcars), rsq) ## [[1]] ## [1] 0.7183433 ## ## [[2]] ## [1] 0.8596865 ## ## [[3]] ## [1] 0.7809306 ## ## [[4]] ## [1] 0.8838038 ## Ex2 cat(&quot;这是Ex2 10个模型的R-square\\n&quot;) ## 这是Ex2 10个模型的R-square lapply(lapply(bootstraps,function(t) lm(mpg~disp,data=t)), rsq) ## [[1]] ## [1] 0.6986607 ## ## [[2]] ## [1] 0.7534348 ## ## [[3]] ## [1] 0.7027078 ## ## [[4]] ## [1] 0.6960993 ## ## [[5]] ## [1] 0.7388991 ## ## [[6]] ## [1] 0.6937985 ## ## [[7]] ## [1] 0.6827093 ## ## [[8]] ## [1] 0.7819097 ## ## [[9]] ## [1] 0.7533153 ## ## [[10]] ## [1] 0.663602 10.2.3 P214-1 Use vapply() to: Compute the standard deviation of every column in a numeric data frame. Compute the standard deviation of every numeric column in a mixed data frame. (Hint: you’ll need to use vapply() twice.) 10.2.3.1 (a) df= data.frame(a = 1:4, b= 5:8, c =9:12) ## 每一列取标准差 vapply(as.list(df),sd,numeric(1)) ## a b c ## 1.290994 1.290994 1.290994 10.2.3.2 (b) df2=data.frame(a = 1:4, b=c(&quot;x&quot;,&quot;y&quot;,&quot;z&quot;,&quot;s&quot;), c =9:12) vapply(df2[vapply(df2, is.numeric, logical(1))], sd, numeric(1)) ## a c ## 1.290994 1.290994 10.2.4 P214-7 Implement mcsapply(), a multicore version of sapply(). Can you implement mcvapply(), a parallel version of vapply()? Why or why not? 10.2.4.1 mcsaplly() 下面分配四个核来构造mcsapply函数 library(parallel) mcsapply &lt;- function(n, func){ core &lt;- makeCluster(4) # 使用4个核 res &lt;- parSapply(core, n, func) # 并行计算，n为次数，func为函数 stopCluster(core) # 关闭核 } 然后不妨尝试还是考虑前面习题中提及的计算置换数据的\\(R^2\\) R2 &lt;- function(i){ index &lt;- sample(1:nrow(mtcars), rep = TRUE) m &lt;- lm(mpg ~ disp, data = mtcars[index,]) return(summary(m)$r.squared) } # 使用sapply函数进行10次 system.time(sapply(1:10, R2)) ## 用户 系统 流逝 ## 0.01 0.00 0.02 # 使用mcsapply函数进行10次 system.time(mcsapply(1:10, R2)) ## 用户 系统 流逝 ## 0.00 0.02 0.14 # 使用sapply函数进行10000次 system.time(sapply(1:10000, R2)) ## 用户 系统 流逝 ## 6.13 0.01 6.14 # 使用mcsapply函数进行10000次 system.time(mcsapply(1:10000, R2)) ## 用户 系统 流逝 ## 0.00 0.02 1.82 实际上，发现当样本量比较小的时候，并行计算反而比不并行所需要的时间更久，因为涉及到分配等额外消耗。当样本量比较大的时候并行计算会快很多。 10.2.4.2 mcvapply() 由于R中并没有现成的parVapply函数，所以并行这一步的处理无法进行 "],["第11次作业解答.html", "11 第11次作业解答 11.1 Question 11.2 Answer", " 11 第11次作业解答 11.1 Question Write an Rcpp function for Exercise 9.8 (page 278, Statistical Computing with R). Compare the corresponding generated random numbers with pure R language using the function “qqplot”. Campare the computation time of the two functions with the function “microbenchmark”. Comments your results. 11.2 Answer This example appears in \\([40]\\). Consider the bivariate density \\[ f(x, y) \\propto\\left(\\begin{array}{l} n \\\\ x \\end{array}\\right) y^{x+a-1}(1-y)^{n-x+b-1}, \\quad x=0,1, \\ldots, n, 0 \\leq y \\leq 1 \\] It can be shown (see e.g. [23]) that for fixed \\(a, b, n\\), the conditional distributions are Binomial \\((n, y)\\) and Beta \\((x+a, n-x+b)\\). Use the Gibbs sampler to generate a chain with target joint density \\(f(x, y)\\). 11.2.1 Gibbs in R a=1 b=1 N=10000 n=25 gibbsR=function(a,b,n,N){ X=matrix(0, N, 2) #样本阵 X[1,]=c(0,0.5) for(i in 2:N){ X2= X[i-1, 2] X[i,1]=rbinom(1,25,X2) X1=X[i,1] X[i,2]=rbeta(1,X1+a,25-X1+b) } return(X) } X=gibbsR(a,b,n,N) plot(X[,1],X[,2],xlab = &quot;x&quot;,ylab = &quot;y&quot;,main = &quot;gibbsR&quot;) 11.2.2 Gibbs in C library(Rcpp) dir=&quot;D:/Software/Github/2021Fall/Statistical_Computing/Homework/hw10/&quot; sourceCpp(paste0(dir,&quot;R.cpp&quot;)) Xc=gibbsC(a,b,n,N) plot(Xc[,1],Xc[,2],xlab = &quot;x&quot;,ylab = &quot;y&quot;,main=&quot;gibbsC&quot;) 11.2.3 QQ plot qqplot(X[,1],Xc[,1],xlab = &quot;gibbsR&quot;,ylab = &quot;gibbsC&quot;,main=&quot;第1维变量QQ图&quot;) abline(0,1,col = &quot;red&quot;) qqplot(X[,2],Xc[,2], xlab = &quot;gibbsR&quot;,ylab = &quot;gibbsC&quot;,main=&quot;第2维变量QQ图&quot;) abline(0,1,col = &quot;red&quot;) Rcpp和R产生的随机数的QQ图基本在一条直线上, 所以产生的随机数的分布基本一致。 11.2.4 Time library(microbenchmark) ts=microbenchmark(gibbR=gibbsR(a,b,n,N), gibbC=gibbsC(a,b,N,n)) summary(ts)[,c(1,3,5,6)] Rcpp运行的平均时间是20ms左右, 但R里运行的时间50000ms左右,Rcpp的效率远高于R "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
